---
title: "Experiment 1: size of the documents"
author: "Erwan Moreau"
date: "2/9/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source('experiments.R')
```

### Links

This document is part of the "CLG Authorship Experiments" repository:

- [Github repository](https://github.com/erwanm/clg-authorship-experiments)
- User guide:
    - [User guide part 1](user-guide-part1.html)
    - [User guide part 2](user-guide-part2.html)
- Experiments:
    - [Experiment 1: size of documents](expe1.html)
    - [Experiment 2: size of a group of documents by the same author](expe2.html)
    - [Experiment 3: number of training cases](expe3.html)
    - [Experiment 4: author diversity in the training set](expe4.html)
    
    

### Options

The `.Rmd` source document can be configured by modifying the following lines:

```{r myoptions}
packages.path <- 'packages'
Sys.setenv(CLG_AUTHORSHIP_PACKAGES_PATH = packages.path)
work.dir <- 'experiments/1-doc-size'
variable.name <- 'Size of documents'
Sys.setenv(EXPE_WORK_DIR = work.dir)
doc.sizes <- '20 40 60 80 100 120 140 160 180 200 300 400 500 600 700 800 900 1000'
Sys.setenv(EXPE1_DOC_SIZES = doc.sizes)
any.doc.size <- strsplit(doc.sizes, ' ',fixed=TRUE)[[1]][1]
set.seed(2022)
```

When reproducing the below experiments manually, one should initialize the environment variables, for instance:

```
export CLG_AUTHORSHIP_PACKAGES_PATH=packages
export EXPE_WORK_DIR=experiments/1-doc-size
export EXPE1_DOC_SIZES='20 40 60 80 100 120 140 160 180 200 300 400 500 600 700 800 900 1000'
```

# Data generation

## Dataset by document size

```{bash generate.data}
source session-setup.sh
[ -d "$EXPE_WORK_DIR" ] || mkdir "$EXPE_WORK_DIR"
for SIZE in $EXPE1_DOC_SIZES; do
  if [ ! -d "$EXPE_WORK_DIR/$SIZE" ]; then 
    echo "Creating data in dir $EXPE_WORK_DIR/$SIZE."
    mkdir "$EXPE_WORK_DIR/$SIZE"
    mkdir "$EXPE_WORK_DIR/$SIZE/data"
    find data/gb/* data/ia/* -maxdepth 0 -type f | grep -v '\.' | ./create-snippets-dataset.sh -r $SIZE "$EXPE_WORK_DIR/$SIZE/data"
  else
    echo "dir $EXPE_WORK_DIR/$SIZE already created."
  fi
done
```


## Training and test cases

```{r generate.cases1}
d <- readDataDir(paste(work.dir,any.doc.size,'data',sep='/'))
dataSplitByAuthor <- splitAuthors(d)
```

```{r generate.cases2}
full <- buildFullDataset(dataSplitByAuthor, 100, 100)
fwrite(full, paste(work.dir,'full-dataset.tsv',sep='/'), sep='\t')
saveDatasetInCasesFormat(full,dir=work.dir)
```

## Adding truth file, impostors and meta-config files

- Simply using all the training docs as impostors.

```{bash truth.files}
source session-setup.sh
for SIZE in $EXPE1_DOC_SIZES; do
  echo "$SIZE: truth file"
  cat "$EXPE_WORK_DIR/train.tsv" > "$EXPE_WORK_DIR/$SIZE/data/truth.txt"
  echo "$SIZE: impostors"
  if [ ! -d "$EXPE_WORK_DIR/$SIZE/impostors" ]; then
    mkdir "$EXPE_WORK_DIR/$SIZE/impostors"
    mkdir "$EXPE_WORK_DIR/$SIZE/impostors/my.impostors"
    cat "$EXPE_WORK_DIR/$SIZE/data/truth.txt" | cut -f 1 | tr ' :' '\n\n'  | sort -u | while read f; do 
      cp "$EXPE_WORK_DIR/$SIZE/data/$f" $EXPE_WORK_DIR/$SIZE/impostors/my.impostors/$f.txt
    done
  fi
  echo "$SIZE: multi-conf files"
  [ -d "$EXPE_WORK_DIR/$SIZE/process" ] || mkdir "$EXPE_WORK_DIR/$SIZE/process"
  generate-multi-conf.sh 1 "$EXPE_WORK_DIR/$SIZE/process"
  cat conf/meta-template.std.multi-conf > "$EXPE_WORK_DIR/$SIZE/process/meta-template.multi-conf"
done
```

# Running the training processes

The script `./run.sh` performs the full training process for one single "size" (variable value). It's a simple script which prepares the data and then starts the training process,  as described in the [user guide (part 2)](user-guide-part2.html). It is used as follows:

```
./run.sh $EXPE_WORK_DIR $SIZE $TASKS_DIR $NCORES
```

- Naturally the training process must be run for every value `$SIZE`. 
- **Caution: A single process takes between 1 and 3 days using 40 cores.**

# Evaluating

The script `./evaluate-all.sh` evaluates:

- for every "size" (variables values),
- the top model (according to the training) for every of the four "model types" (basic, GI, univ, meta),
- on both the training and test set,
- and calculates the "author seen/unseen" performance values.

It is used as follows:

```
./evaluate-all.sh $EXPE_WORK_DIR $NCORES $TASKS_DIR
```

* The evaluation process is also resource-intensive, it takes up to 3 hours with 40 cores, depending on the number of values.
* The process creates the directory `$EXPE_WORK_DIR/results` which contains the detailed output for every evaluated model. 
    * The main output is contained in `$EXPE_WORK_DIR/results/results.tsv`.


# Analysis


```{r loading}
d<-readExperimentResults(work.dir)
```

```{r graph1}
g1 <- perfByModelType(d,x.label=variable.name)
g1
```


```{r graph2}
g2 <- comparePerfsByEvalOn(d,diff.seen=FALSE,x.label=variable.name)
g2
```

```{r graph3}
g3 <- comparePerfsByEvalOn(d,diff.seen=TRUE,x.label=variable.name)
g3
```

```{r save.graphs}
g<-plot_grid(g1,g2,g3,labels=NULL,ncol=3)
ggsave('graphs-expe1.pdf',g,width=30,height=8,unit='cm')
```

Below the same graphs are split by size for the sake of readability.

## Size below 200 by steps of 20

```{r loadingA}
d<-readExperimentResults(work.dir)
d <- d[variable<=200,]
```

```{r graph1A}
g1 <- perfByModelType(d,x.label=variable.name)
g1
```


```{r graph2A}
g2 <- comparePerfsByEvalOn(d,diff.seen=FALSE,x.label=variable.name)
g2
```

```{r graph3A}
g3 <- comparePerfsByEvalOn(d,diff.seen=TRUE,x.label=variable.name)
g3
```

```{r save.graphsA}
g<-plot_grid(g1,g2,g3,labels=NULL,ncol=3)
ggsave('graphs-expe1A.pdf',g,width=30,height=8,unit='cm')
```


## Size between 100 and 1000 by steps of 100

```{r loadingB}
d<-readExperimentResults(work.dir)
d <- d[variable>=200 | variable ==100,]
```

```{r graph1B}
g1 <- perfByModelType(d,x.label=variable.name)
g1
```


```{r graph2B}
g2 <- comparePerfsByEvalOn(d,diff.seen=FALSE,x.label=variable.name)
g2
```

```{r graph3B}
g3 <- comparePerfsByEvalOn(d,diff.seen=TRUE,x.label=variable.name)
g3
```

```{r save.graphsB}
g<-plot_grid(g1,g2,g3,labels=NULL,ncol=3)
ggsave('graphs-expe1B.pdf',g,width=30,height=8,unit='cm')
```
