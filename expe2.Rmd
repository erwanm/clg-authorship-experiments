---
title: "Experiment 2: size of a group of documents by the same author"
author: "Erwan Moreau"
date: "2/9/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source('experiments.R')
```

### Links

This document is part of the "CLG Authorship Experiments" repository:

- [Github repository](https://github.com/erwanm/clg-authorship-experiments)
- User guide:
    - [User guide part 1](user-guide-part1.html)
    - [User guide part 2](user-guide-part2.html)
- Experiments:
    - [Experiment 1: size of documents](expe1.html)
    - [Experiment 2: size of a group of documents by the same author](expe2.html)
    - [Experiment 3: number of training cases](expe3.html)
    - [Experiment 4: author diversity in the training set](expe4.html)
    
    

### Options

The `.Rmd` source document can be configured by modifying the following lines:

```{r myoptions}
analyze.results <- TRUE
packages.path <- 'packages'
Sys.setenv(CLG_AUTHORSHIP_PACKAGES_PATH = packages.path)
work.dir <- 'experiments/2-doc-groups-by-case'
variable.name <- 'Documents by group'
Sys.setenv(EXPE_WORK_DIR = work.dir)
Sys.setenv(COPY_EXPE1_DATA_DIR = 'experiments/1-doc-size/100')
group.sizes <- '1 2 3 4 5'
Sys.setenv(GROUP_SIZES = group.sizes)
any.group.size <- strsplit(group.sizes, ' ',fixed=TRUE)[[1]][1]
set.seed(2022)
```

When reproducing the below experiments manually, one should initialize the environment variables, for instance:

```
export CLG_AUTHORSHIP_PACKAGES_PATH=packages
export EXPE_WORK_DIR=experiments/2-doc-groups-by-case
export COPY_EXPE1_DATA_DIR=experiments/1-doc-size/100
export GROUP_SIZES='1 2 3 4 5'
```


# Data generation

## Dataset 

We use the same dataset as in the first experiment (must have been calculated before).

```{bash generate.data}
source session-setup.sh
if [ ! -d "$COPY_EXPE1_DATA_DIR" ]; then
  echo "Dir '$COPY_EXPE1_DATA_DIR' not found" 1>&2
  exit 1
fi
if [ ! -d "$EXPE_WORK_DIR" ]; then 
  mkdir "$EXPE_WORK_DIR"
  for SIZE in $GROUP_SIZES; do
     mkdir "$EXPE_WORK_DIR/$SIZE"
     cp -R "$COPY_EXPE1_DATA_DIR"/process "$COPY_EXPE1_DATA_DIR"/data "$COPY_EXPE1_DATA_DIR"/impostors "$EXPE_WORK_DIR/$SIZE"
     rm -f "$EXPE_WORK_DIR/$SIZE/data/truth.txt"
  done
fi
```


## Training and test cases

```{r generate.cases1}
d <- readDataDir(paste(work.dir,any.group.size,'data',sep='/'))
dataSplitByAuthor <- splitAuthors(d)
```

```{r generate.cases2}
for (size.str in strsplit(group.sizes, ' ',fixed=TRUE)[[1]]) {
  size <- as.numeric(size.str)
  full <- buildFullDatasetWithGroups(dataSplitByAuthor, 100, 100,group1.sizes=size, group2.sizes=1)
  fwrite(full, paste(work.dir,size.str,'full-dataset.tsv',sep='/'), sep='\t')
  saveDatasetInCasesFormat(full,dir=paste(work.dir,as.character(size),sep='/'))
}
```

## Adding truth file

```{bash truth.files}
source session-setup.sh
for SIZE in $GROUP_SIZES; do
  echo "$SIZE: truth file"
  cat "$EXPE_WORK_DIR/$SIZE/train.tsv" > "$EXPE_WORK_DIR/$SIZE/data/truth.txt"
done
```

# Running the training processes

The script `./run.sh` performs the full training process for one single "size" (variable value). It's a simple script which prepares the data and then starts the training process,  as described in the [user guide (part 2)](user-guide-part2.html). It is used as follows:

```
./run.sh $EXPE_WORK_DIR $SIZE $TASKS_DIR $NCORES
```

- Naturally the training process must be run for every value `$SIZE`. 
- **Caution: A single process takes between 1 and 3 days using 40 cores.**

# Evaluating

The script `./evaluate-all.sh` evaluates:

- for every "size" (variables values),
- the top model (according to the training) for every of the four "model types" (basic, GI, univ, meta),
- on both the training and test set,
- and calculates the "author seen/unseen" performance values.

It is used as follows:

```
./evaluate-all.sh $EXPE_WORK_DIR $NCORES $TASKS_DIR
```

* The evaluation process is also resource-intensive, it takes up to 3 hours with 40 cores, depending on the number of values.
* The process creates the directory `$EXPE_WORK_DIR/results` which contains the detailed output for every evaluated model. 
    * The main output is contained in `$EXPE_WORK_DIR/results/results.tsv`.


# Analysis


```{r loading,eval=analyze.results}
d<-readExperimentResults(work.dir)
```

```{r graph1,eval=analyze.results}
g1 <- perfByModelType(d,x.label=variable.name)
g1
```


```{r graph2,eval=analyze.results}
g2 <- comparePerfsByEvalOn(d,diff.seen=FALSE,x.label=variable.name)
g2
```

```{r graph3,eval=analyze.results}
g3 <- comparePerfsByEvalOn(d,diff.seen=TRUE,x.label=variable.name)
g3
```

```{r save.graphs,eval=analyze.results}
g<-plot_grid(g1,g2,g3,labels=NULL,ncol=3)
ggsave('graphs-expe2.pdf',g,width=30,height=8,unit='cm')
```


