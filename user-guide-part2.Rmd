---
title: "CLG authorship analytics - user guide (part 2)"
author: "Erwan Moreau"
date: "1/23/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Links

This document is part of the "CLG Authorship Experiments" repository:

- [Github repository](https://github.com/erwanm/clg-authorship-experiments)
- User guide:
    - [User guide part 1](user-guide-part1.html)
    - [User guide part 2](user-guide-part2.html)
- Experiments:
    - [Experiment 1: size of documents](expe1.html)
    - [Experiment 2: size of a group of documents by the same author](expe2.html)
    - [Experiment 3: number of training cases](expe3.html)
    - [Experiment 4: author diversity in the training set](expe4.html)
    
    


# Overview

This is the second part of the user guide for "CLG authorship analytics". The first part is [here](user-guide-part1.html).

Note: this guide is organized by order of complexity, from the most simple to the most complex kind of process. It is meant to help the reader understand what each of the different levels of the process actually does. Thus the simplest way to run the full process is described at the end of this document. 


## Rmd

This document was generated from an [R Markdown](https://rmarkdown.rstudio.com/) source file. The source file is provided in the repository, it can be used to reproduce these experiments. It can be executed through the RStudio interface ("knit" button) or as follows:

```
rmarkdown::render('user-guide-part2.Rmd')
```

- Naturally the document can also be read as a regular documentation.
- **Important:** this Rmd document includes [bash chunks](https://bookdown.org/yihui/rmarkdown/language-engines.html). In order for these to work, the environment must have been configured as explained in [Requirements](#requirements) below.

### Options

The `.Rmd` source document can be configured by modifying the following lines:

```{r myoptions}
packages.path <- 'packages'
Sys.setenv(CLG_AUTHORSHIP_PACKAGES_PATH = packages.path)
output.dir <- '/tmp/user-guide-part2.output'
Sys.setenv(OUTPUT_DIR = output.dir)
delete.previous.output <- TRUE
snippets.size <- 25
Sys.setenv(SNIPPETS_SIZE = snippets.size)
Sys.setenv(NCORES = 40)
Sys.setenv(TASKS_DIR= '/tmp/clg-authorship.tasks')
Sys.setenv(EXPE_WORK_DIR = 'experiments/1-doc-size')
```

* Note: `ncores` should be set to represent the number of cores available on the machine.
* The role of `TASKS_DIR` is described below in the "Parallel/distributed processing" section.

In order to manually execute some of the commands below, it is recommended to assign the appropriate value to the above environment variables for the whole session, for example:

```{bash init0}
export CLG_AUTHORSHIP_PACKAGES_PATH=packages
export OUTPUT_DIR=/tmp/user-guide-part1.output
```

### Initialization

```{bash init1,echo=delete.previous.output,eval=delete.previous.output}
rm -rf "$OUTPUT_DIR" "$TASKS_DIR"
```

```{bash init2}
mkdir "$OUTPUT_DIR"
mkdir "$TASKS_DIR"
```

```{bash snippets}
source session-setup.sh
find data/gb/* data/ia/* -maxdepth 0 -type f | grep -v '\.' | ./create-snippets-dataset.sh $SNIPPETS_SIZE $OUTPUT_DIR/data
```


# Requirements

The software and data requirements are the same as described in [user guide part 1](user-guide-part1.html#requirements), except that **this part requires a lot more computational power**. It is strongly recommended to run these experiments on a machine with many cores and a large amount of RAM available.


# Parallel/distributed processing

The following examples require parallel procesing. The scripts do not handle this themselves, instead a system of external parallel processing is used:

- A predetermined directory, for instance `~/tasks`, is used as an inbox for the tasks to process.
- A script run with argument `-P ~/tasks` writes multiple individual tasks as simple one-line scripts into this directory, instead of running them sequentially.
- An external script [`task-distrib-daemon.sh`](https://erwanm.github.io/erw-bash-commons/#task-distrib-daemon.sh) runs in the background, picks up the tasks written in `~/tasks` and runs them in parallel.
    - `task-distrib-daemon.sh` accepts various options which makes it possible to use for distributed processing on multiple machine (typically on a computing cluster).
    - Tasks are run whenever a slot is available, based on the provided number of cores.
- The calling script waits for the tasks to be achieved by checking regularly if some output file has been written.
- This system allows flexibility at various levels. In particular multiple scripts can use to the same `~/tasks` directory, as long as they write uniquely named files. Since the daemon runs globally, there is no need to control the number of cores allocated to each script. 
    - This is used to start the genetic process (see below) for several strategies running at the same time, each of them leaving their tasks in the same directory.



# Genetic learning

Naturally there is a high number of possible configurations. The system is intended to learn which configuration(s) work best with a training dataset, using a genetic algorithm to explore the space of possible configurations efficiently.

## Multi-configurations files

The concept of *multi-config file* is introduced for the genetic process. Such a config file is meant to enumerate all the possible values for a parameter: 

```
paramName=value1 value2 ... valueN
``` 

The multi-conf contains the parameters for which the Genetic process has to select the optimal values. The parameters are grouped into three categories (see the [documentation](https://erwanm.github.io/clg-authorship-analytics/#ConfigurationFile) for mor details):

- Common parameters
- Strategy-specific parameters
- Genetic process parameter

Examples of multi-config "parts" files are provided in `conf/` These different parts can be combined:

```{bash multi2}
source session-setup.sh
mkdir "$OUTPUT_DIR/gen1"
generate-multi-conf.sh -c common.simplified.multi-conf.part -g genetic.simplified.multi-conf.part 0 "$OUTPUT_DIR/gen1/0"
```


- By default the configs parts are found in the directory `./conf` . The `-d` option can be used to specify the directory where the configuration "parts" are found. 
- `-c` and  `-g` are used to force the use of a specific config file for the 'common' and 'genetic' part instead of the default, in this case a simplified variant meant to take less time  (see also [below](#genetic-algorithm) about the genetic process configuration). 
- The first mandatory argument is 0 for ignoring observation types involving POS tags, 1 otherwise.
- This generates a full multi-config file on `$OUTPUT_DIR/gen1/0/multi-conf-files` for every strategy.
- The subdir `0` is needed to avoid that `genetic-train.sh` deletes the dir (confusing design)

### Generating individual config files from a multi-config

The script `expand-multi-config.pl` is used to generate a subset of individual config files based on a multi-config.

- By default it generates the exhaustive set of all the combinations of parameters values. Of course, **using this option wouldn't be wise with a very large number of parameters/values** (see below the number with the `basic` multi-conf file)
- With option `-r <N>` (used in the example below) it generates a random subset of N configs.
- With option `-g` (genetic algorithm) it generates a new generation of configs based on the results of a previous generation. This is called internally in the script `train-genetic.sh` presented later.

```{bash multi3}
source session-setup.sh
targetdir="$OUTPUT_DIR/example-50-configs"
mkdir "$targetdir"
echo "$OUTPUT_DIR/gen1/0/multi-conf-files/basic.multi-conf"  | expand-multi-config.pl -r 50 "$targetdir/"
```


## Preparing the input data



In [part 1](user-guide-part1.html) we were manually "preparing" the input data in the format required by the system.
The script `prepare-input-data.sh` can be used to create the working directory with all the required parts in a way which makes it usable directly by the training scripts: 

* Copies the input data (to avoid modifying the input directory)
* Stores the multi-config files
* Calculates the ["stop words"](user-guide-part1.html#stop-words-for-word-observations) based on the training set documents (unless `-r` is supplied) and stores them where needed
    * The "stop words limits" are obtained directly from the required obs types, assuming that the "stop word id" is a simple integer.
* Prepares all obs types for the input data, including POS if required
* The impostors data must be provided:
    * either as a "prepared resource" with `-r`
    * or as simple files in a directory with `-i`
        * in this case the impostors data is prepared, i.e. the required obs types are calculated
* In any case the impostors pre-similarity values are computed against every document in the input data
* The resources options file is generated




### Input data

Below we prepare some data using a training and test set generated for the main experiments proposed in this repository (see [Experiment 1: size of documents](expe1.html)).

```{bash gen.init}
mkdir "$OUTPUT_DIR"/input-data.gen1 
cat "$EXPE_WORK_DIR/train.tsv" > "$OUTPUT_DIR/input-data.gen1/truth.txt"
cat "$EXPE_WORK_DIR/train.tsv" | cut -f 1 | tr ' :' '\n\n'  | sort -u | while read f; do 
      cp "$OUTPUT_DIR/data/$f" "$OUTPUT_DIR/input-data.gen1/"
done
head -n 50 "$EXPE_WORK_DIR/train.tsv" > "$OUTPUT_DIR/gen1.train-cases.txt"
tail -n 50 "$EXPE_WORK_DIR/train.tsv" > "$OUTPUT_DIR/gen1.test-cases.txt"
```

- Note: the script expects a file `truth.txt` in a subdirectory `input` containing the cases to be used for training.
- In this example the training set is split into two parts in order to use the second part as validation set (see below).
    - The file `truth.txt` must contain all the cases.


### Impostors data


For this toy example we also create a small set of impostors in the same way as [presented in the GI method](user-guide-part1.html#preparing-the-impostors)):


```{bash gen.prep2}
source session-setup.sh
mkdir $OUTPUT_DIR/impostors.gen1
mkdir $OUTPUT_DIR/impostors.gen1/GI.1.impostors
n=$(find $OUTPUT_DIR/data/* -maxdepth 0 -type f | wc -l)
find $OUTPUT_DIR/data/* -maxdepth 0 -type f | random-lines.pl 100 1 $n | while read f; do cp $f $OUTPUT_DIR/impostors.gen1/GI.1.impostors/$(basename "$f").txt; done
```


- Note: the name `my.impostors` is the one used in the default GI config file `conf/GI.multi-conf.part`.



### Running the preparation script


- Language: By default a file `contents.json` is expected in the input data dir (PAN input format), and the language is extracted from it. Option `-l` is used as an alternative to the json file.
- `-i` specifies where to find the impostors dataset(s)
- In this case we only provide the "basic" multi-conf file to the script on STDIN. We will show later a more general way.

```{bash prepare.input}
source session-setup.sh
echo "$OUTPUT_DIR/gen1/0/multi-conf-files/basic.multi-conf"  | prepare-input-data.sh -l english -i $OUTPUT_DIR/impostors.gen1/ "$OUTPUT_DIR/input-data.gen1" "$OUTPUT_DIR/gen1"
```


## Genetic algorithm

### Configuration

The genetic process is controlled by the "genetic" part of the multi-configuration:

```{bash gen.config1}
cat conf/genetic.simplified.multi-conf.part
```

- The `population` parameter is the number of individual configs in every generation.
- The "stop criterion" defines when to stop the genetic process: the average performance of the configs in a generation is averaged over a window of the N last generations (`stopCriterionNbGenerationsByWindow`). The process stops if this value does not increase during M generations (`stopCriterionNbWindows`). 
    - Note: this means that the process must run at least N generations.
- The `geneticParams` are 4 values separated by colons:
    - `prop breeders` is the proportion of indivuals to select from the prev gen (cannot be higher than 0.5)
    - `prob mutation` is the probablity of a mutation on one gene. This is actually the probability of a new random value, which can end up being one the parents values.
    - `prop elite` is the proportion of the new generation taken as is from the top previous generation
    - `prop random` is the proportion of the new generation selected totally randomly.
- `perfCriterion`: evalation measure to use: `auc`, `C1` (C@1, variant of accuracy with 'unsure' answer) or `final` (product of the previous two, as used in PAN15).
- `nbFoldsOrProp`: $k$ value for $k$-fold cross-validation
- `returnNbBest`: number $n$ of top $n$ best performing configs to return as output of the full process.

The full genetic process may involve a sequence of several steps, each identified with a prefix (e.g. `indivGenetic_`) and followed by an integer or `final` for the last step. This allows progressively refining the evaluation of the configs, for instance by using a higher $k$ or larger 'stop criterion window' in the later stages of the process. This multi-steps sequence is managed by [`train-multi-stages.sh`](https://erwanm.github.io/clg-authorship-analytics/#Implementation%3A%20train-multi-stages.sh) (see also below).



### Running the genetic process


```{bash gen.go}
source session-setup.sh
task-distrib-daemon.sh  -s 30s -p 1 -v -q 4 "$TASKS_DIR" $NCORES >"$OUTPUT_DIR/gen1.task-daemon.log" &
echo "$OUTPUT_DIR/gen1/0/multi-conf-files/basic.multi-conf" | train-genetic.sh -P "$TASKS_DIR/mytasks" -o  '-c -s' "$OUTPUT_DIR/gen1" "$OUTPUT_DIR/gen1.train-cases.txt" indivGenetic_1_
```

- The above process takes around 10 to 20 mn with 40 cores.
- To see whether the daemon is starting tasks as expected, check its output in `$OUTPUT_DIR/gen1.task-daemon.log`.
    - Note: the deamon outputs a summary every 30 minutes by default. The duration, level of detail and various options can be configured in the parameters (see `task-distrib-daemon.sh -h`):
        - `-s <duration>` cycle duration, i.e. how often to check for new tasks.
        - `-p <N>` print summary every N cycles.
        - `-v` verbose mode, more details printed (`-V` for highest verbosity level).
        - `-q <N>` stop the daemon after N cycles idle (no task running, no new task). By default the daemon runs until interrupted.
- The option `-o '-c -s'` is used to call `train-cv.sh` with the following two options:
    - `-c` cleans up intermediate files created by the process 
    - `-s` (for *safe*) prevents errors from causing the full process to abort. This is useful as the randomness of the process may sometimes lead to inconsistent input, causing errors. But it should be used with caution, since this implies that errors are ignored.
- Errors happening in the individual processes are written to files named `$OUTPUT_DIR//gen1/generations/*/train/*.log.err`, where the first `*` corresponds to the geneation id and the second `*` to the configuration id in this generation.


### Multi-stages genetic process

As mentioned [above](#configuration), there can be several steps of genetic training with different parameters. A multi-stages sequence must always end with a special stage identified as `final` in the genetic configuration file. This step finalizes the process:

- It performs [5x2 repeated cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Repeated_random_sub-sampling_validation) with [train-multi-runs.sh](https://erwanm.github.io/clg-authorship-analytics/#5x2%20cross-validation) on the best configurations returned by the last stage of genetic training. The final set of configs is selected based on the results of this evaluation.
- It re-trains the selected configs on the full training data and also performs once cross-validation once again to obtain the final evaluation scores.

In the following code we use the `-r` option to resume the previous training and apply the final stage. Note that the id  provided as last parameter contains only the prefix `indivGenetic` since it's not about a specific stage anymore. 

```{bash gen.multi}
#source session-setup.sh
task-distrib-daemon.sh  -s 30s -p 1 -v -q 4 "$TASKS_DIR" $NCORES >"$OUTPUT_DIR/gen1.task-daemon.log" &
train-multi-stages.sh -r -P "$TASKS_DIR/mytasks" -o  '-c -s' "$OUTPUT_DIR/gen1" "$OUTPUT_DIR/gen1.train-cases.txt" "$OUTPUT_DIR/gen1/0/multi-conf-files/basic.multi-conf" indivGenetic
```

# Meta-classifier (stacked generalization)

We have seen above that the genetic process can return a set of N best performing configurations. Additionally different strategies can be trained, every strategy returning its N best configurations. These individual configs can be considered as individual learners and combined in a meta-classifier which leverages their diversity to predict the final results (hopefully more accurately that any individual learner).

The training of the meta-model is done with the same genetic algorithm as used above, where the parameters to optimize are now the subset of individual configs/models and possibly some additional parameters for the meta-model. Since the set of possible individual models is fixed,  the predictions that they return are precomputed in order to avoid repeatedly computing them during the genetic training. Note that this makes this genetic learning stage much faster than the previous one, but it may in some cases cause a bias due to the randomness of some of the methods (since the individual learners are treated as deterministic at this stage).

### `train-outerCV-1fold.sh`

The meta-training is performed by `train-outerCV-1fold.sh`, which takes as input a training set and a test set. The script uses them in a quite complex way explained below:

- The training set (called "strategy training set" later) is first used to perform the genetic training presented above by calling `train-multi-stages.sh` *for every strategy*.
- The N best configs for every strategy are obtained from the stage above. These configs are applied to both the training and test set instances (with `apply-multi-configs.sh`).
- The test set is then randomly split into two subsets:
    - meta-training set
    - meta validation set (or meta-test set)
- The meta-training set is used to perform the genetic training of the meta-model, calling `train-multi-stages.sh` again (only once).
- The N best configurations obtained from the meta-training are applied to all the instances: strategy training set, meta-training set and validation set. These top configurations are also "packaged" in a sub directory `refactored-strategy-configs` in order to be easily usable for application later.
- Bagging ([bootstrap aggregating](https://en.wikipedia.org/wiki/Bootstrap_aggregating)) is applied on the meta validation set (fresh instances) in order to select the final top N meta-configs. This step can be used to analyze the variance of the meta-configs.


For the sake of simplicity and computation speed, in the example below we remove the `GI` and `univ` strategies before running the process, resulting in the meta-classifier using only the `basic` strategy. This is of course not necessary nor recommended in real applications. 


```{bash gen.meta.init2}
cp conf/meta-template.simplified.multi-conf "$OUTPUT_DIR/gen1/meta-template.multi-conf"
# below removing univ and GI strategies to make the process faster
rm -f "$OUTPUT_DIR/gen1/0/multi-conf-files/GI.multi-conf" "$OUTPUT_DIR/gen1/0/multi-conf-files/univ.multi-conf"
```

```{bash gen.meta}
source session-setup.sh
task-distrib-daemon.sh  -s 30s -p 1 -v -q 4 "$TASKS_DIR" $NCORES >"$OUTPUT_DIR/gen1.task-daemon.log" &
train-outerCV-1fold.sh -r -P "$TASKS_DIR/mytasks" -o  '-c -s' "$OUTPUT_DIR/gen1" "$OUTPUT_DIR/gen1.train-cases.txt" "$OUTPUT_DIR/gen1.test-cases.txt" "$OUTPUT_DIR/gen1/0/multi-conf-files"
```



# Full example

This part shows the full process of training a meta-model with several strategies, albeit with simplified parameters. This experiment is self-contained, i.e. it doesn't rely on any part executed previously. See above for the detailed explanations.

## Preparing the input data

### Input data

We use the training set generated for the main experiments (see [Experiment 1: size of documents](expe1.html)).

```{bash full.prepa1}
mkdir "$OUTPUT_DIR"/input-data.full
cat "$EXPE_WORK_DIR/train.tsv" > "$OUTPUT_DIR/input-data.full/truth.txt"
cat "$EXPE_WORK_DIR/train.tsv"  | cut -f 1 | tr ' :' '\n\n'  | sort -u | while read f; do
      cp "$OUTPUT_DIR/data/$f" "$OUTPUT_DIR/input-data.full/"
done
```

## Impostors data

We build a small impostors dataset (same way as above):

```{bash full.prepa2}
source session-setup.sh
mkdir $OUTPUT_DIR/impostors.full
mkdir $OUTPUT_DIR/impostors.full/my.impostors
n=$(find $OUTPUT_DIR/data/* -maxdepth 0 -type f | wc -l)
find $OUTPUT_DIR/data/* -maxdepth 0 -type f | random-lines.pl 100 1 $n | while read f; do cp $f $OUTPUT_DIR/impostors.full/my.impostors/$(basename "$f").txt; done
```

- Note: the name `my.impostors` is the one used in the default GI config file `conf/GI.multi-conf.part`.

### Multi-conf files

For this example we create some simplified multi-conf files which are slightly more advanced than in the previous example:

- The three strategies are included.
- Observation types with stop word and POS tags are used.

**Important:** The "meta" config file must also be copied in the target directory under the name `meta-template.multi-conf`, as shown below.

```{bash full.multi2}
source session-setup.sh
mkdir "$OUTPUT_DIR/full"
generate-multi-conf.sh -c common.simplified2.multi-conf.part -g genetic.simplified.multi-conf.part 1 "$OUTPUT_DIR/full"
cat conf/meta-template.simplified.multi-conf > "$OUTPUT_DIR/full/meta-template.multi-conf"
```



### Running the preparation script


```{bash full.prepare.input}
source session-setup.sh
prepare-input-data.sh -l english -i $OUTPUT_DIR/impostors.full/ "$OUTPUT_DIR/input-data.full" "$OUTPUT_DIR/full"
```


## Running the full training process with `train-top-level.sh`


Compared to `train-outerCV-1fold.sh`, The script `train-top-level.sh` can be used to run an additional level of cross-validation... although it does not actually perform a proper cross-validation:

- The training data is split randomly into two subsets  and B. `train-outerCV-1fold.sh` is called twice: 
     - With A as strategy training set and  as meta-training set + validation set, 
     - With B as strategy training set and A as meta-training set + validation set.
- After extracting/copying the best models (config and possibly Weka model) from both folds, the final bagging stage is performed on the whole training data. 
- The best models are sorted by their performance (depending on the selection method given as parameter, either on the validation set (meta-test set), full training set, or average of both) and written to a file named `best.results`, in which lines follow the format: `<Perf full training set> <Perf meta test set> <Average both>`.

This "top-level" stage can be useful to manually assess the effect of randomness on the results (to some extent). However the double genetic process is unlikely to return the exact same configurations in the two runs, so it is impossible in general to calculate the average performance of a particular configuration across the runs. 

The script receives only the previously initialized working directory, which contains the training data and multi-configuration files. 

```{bash full.run}
source session-setup.sh
task-distrib-daemon.sh  -s 30s -p 1 -v -q 10 "$TASKS_DIR" $NCORES >"$OUTPUT_DIR/full.task-daemon.log" &
train-top-level.sh -r -P "$TASKS_DIR/mytasks" -o  '-c -s' "$OUTPUT_DIR/full"
```


## Extracting the best model 

At the end of the training process, the identifiers for the top N models/configurations (N=100 with the default parameters) found by the training process are written to the file `best.results` in the output directory. The models are ordered by decreasing performance on the validation set (meta test set; this depends on the parameters), thus showing the top model on the first line.

* The three performance values available for every model in `best.results` are: 
    * performance on the validation set (meta test set)
    * performance on the full training set
    * average of the two above values
* Additional performance statistics are available for the top models in `bagging-meta-test-fold-all.stats`, which contains four values for every model: 
    * mean 
    * median 
    * standard deviation 
    * mean minus std. dev.


```{bash}
head "$OUTPUT_DIR/full/best.results" 
```

The actual models corresponding to these identifiers are stored in the subdirectory `selected-meta-configs/`. The id contained in `best.results` actually corresponds to the full pathname prefix, so that:

* `<prefix>.conf` is the configuration file for the model
* `<prefix>.model` is the directory containing the model.

If provided with such a patname prefix, the system can apply the corresponding model. Of course the path needs to exist and contain the model.


## Testing with fresh instances



### Preparing the test instances

When testing with fresh instances, the corresponding documents must have been "prepared" (their observation files computed, etc.) before applying the model per se. Of course this is not necessary if the documents in the test set have been prepared previously, together with the training documents (see above).

We use the test set generated for the main experiments (see [Experiment 1: size of documents](expe1.html)). Here we just copy the test files:

```{bash test.prepa1}
mkdir "$OUTPUT_DIR"/test-data.full
cat "$EXPE_WORK_DIR/test.tsv" > "$OUTPUT_DIR/test-data.full/truth.txt"
cat "$EXPE_WORK_DIR/test.tsv" | cut -f 1 | tr ' :' '\n\n'  | sort -u | while read f; do
      cp "$OUTPUT_DIR/data/$f" "$OUTPUT_DIR/test-data.full/"
done
```

Then we call the script `prepare-input-data.sh` on the directory containing the test documents:

```{bash test.prepare.input}
source session-setup.sh
ls $OUTPUT_DIR/full/multi-conf-files/*multi-conf | prepare-input-data.sh -l english -r $OUTPUT_DIR/full/resources "$OUTPUT_DIR/test-data.full" "$OUTPUT_DIR/full.test"
```

- It's important to use the same impostors documents and list of stop words as for the training set. This is why the `-r` option allows to provide an exsting "resources" directory.
- Note: it's also possible to add the documents to the directory used for training.



### Applying the model to the test instances

Below the best model found by the training stage is extracted, then the id (path prefix) is passed to the script `test-meta-model.sh`:


```{bash test.run}
source session-setup.sh
bestModel=$(head -n 1 "$OUTPUT_DIR/full/best.results" | cut -f 1)
echo "Model id: $bestModel"
task-distrib-daemon.sh  -s 30s -p 1 -v -q 4 "$TASKS_DIR" $NCORES >"$OUTPUT_DIR/test.task-daemon.log" &
test-meta-model.sh -p  -P "$TASKS_DIR/mytasks" "$bestModel" "$EXPE_WORK_DIR/test.tsv" "$OUTPUT_DIR/full.test" "$OUTPUT_DIR/full.test.results" 
```

- The `-p` option requires computing performance. The script can also be used to only predict the answers for unlabelled cases.


- The output directory contains:
    - a file `$bestModel.answers` with the predicted probability for every case in the test set
    - a file `$bestModel.perf` with the test set performance if `-p` was used.

Performance on the test set:

```{bash test.perf}
cat "$OUTPUT_DIR/full.test.results"/*.perf
```

