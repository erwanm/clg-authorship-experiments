---
title: "CLG authorship analytics - user guide (part 2)"
author: "Erwan Moreau"
date: "1/23/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Overview

This is the second part of the user guide for "CLG authorship analytics". The first part is [here](user-guide-part1.html).

## Rmd

This document was generated from an [R Markdown](https://rmarkdown.rstudio.com/) source file. The source file is provided in the repository, it can be used to reproduce these experiments. It can be executed through the RStudio interface ("knit" button) or as follows:

```
rmarkdown::render('user-guide-part2.Rmd')
```

- Naturally the document can also be read as a regular documentation.
- **Important:** this Rmd document includes [bash chunks](https://bookdown.org/yihui/rmarkdown/language-engines.html). In order for these to work, the environment must have been configured as explained in [Requirements](#requirements) below.

### Options

The `.Rmd` source document can be configured by modifying the following lines:

```{r myoptions}
packages.path <- 'packages'
Sys.setenv(CLG_AUTHORSHIP_PACKAGES_PATH = packages.path)
output.dir <- 'user-guide-part2.output'
Sys.setenv(OUTPUT_DIR = output.dir)
delete.previous.output <- TRUE
# normally not needed with regular setup:
Sys.setenv(ADD_TO_PERL5LIB = '/home/moreaue/perl5/lib/perl5')
```

See [user guide part 1](user-guide-part1.html#options) for details.

### Initialization

```{bash init1,echo=delete.previous.output,eval=delete.previous.output}
rm -rf "$OUTPUT_DIR"
```

```{bash init2}
mkdir "$OUTPUT_DIR"
```

# Requirements

The software and data requirements are the same as described in [user guide part 1](user-guide-part1.html#requirements), except that **this part requires a lot more computational power**. It is strongly recommended to run these experiments on a machine with many cores and a large amount of RAM available.

# Genetic learning

Naturally there is a high number of possible configurations. The system is intended to learn which configuration(s) work best with a training dataset, using a genetic algorithm to explore the space of possible configurations efficiently.

## Multi-configurations files

The concept of *multi-config file* is introduced for the genetic process. Such a config file is meant to enumerate all the possible values for a parameter: 

```
paramName=value1 value2 ... valueN
``` 

The multi-conf contains the parameters for which the Genetic process has to select the optimal values. The parameters are grouped into three categories (see the [documentation](https://erwanm.github.io/clg-authorship-analytics/#ConfigurationFile) for mor details):

- Common parameters
- Strategy-specific parameters
- Genetic process parameter

Examples of multi-config "parts" files are provided in `configs/multi/` These different parts can be combined:

```{bash multi2}
source session-setup.sh
mkdir "$OUTPUT_DIR/gen1"
generate-multi-conf.sh -d configs/multi/ 0 "$OUTPUT_DIR/gen1/0"
```

- The second argument is 0 for ignoring obsertvation types involving POS tags, 1 otherwise.
- This generates a full multi-config file on `$OUTPUT_DIR/gen1/0/multi-conf-files` for every strategy.
- The subdir `0` is needed to avoid that `genetic-train.sh` deletes the dir (confusing design)

### Generating individual config files from a multi-config

The script `expand-multi-config.pl` is used to generate a subset of individual config files based on a multi-config.

- By default it generates the exhaustive set of all the combinations of parameters values. Of course, **using this option wouldn't be wise with a very large number of parameters/values** (see below the number with the `basic` multi-conf file)
- With option `-r <N>` (used in the example below) it generates a random subset of N configs.
- With option `-g` (genetic algorithm) it generates a new generation of configs based on the results of a previous generation. This is called internally in the script `train-genetic.sh` presented later.

```{bash multi3}
source session-setup.sh
targetdir="$OUTPUT_DIR/example-50-configs"
mkdir "$targetdir"
#echo "$OUTPUT_DIR/gen1/0/multi-conf-files/basic.multi-conf"  | expand-multi-config.pl -r 50 "$targetdir/"
```


## Genetic algorithm

### Preparation

The multi-config files above contain some ["stop-words" observation types](user-guide-part1.html#stop-words-for-word-observations). For the sake of simplicity we just use the Mark Twain books to compute two stop words lists (50 and 200):


```{bash gen.prep1}
source session-setup.sh
find data/gb/*-mt-* -maxdepth 0 -type f >"$OUTPUT_DIR/mark-twain.list"
count-obs-dataset.sh -i "$OUTPUT_DIR/mark-twain.list" -o '-g' english WORD.T.lc1.sl0.mf2 2>/dev/null
sort -r -n +1 -2 "$OUTPUT_DIR/global.observations/WORD.T.lc1.sl0.mf2.count" | cut -f 1 | head -n 50 >"$OUTPUT_DIR/mt-50.stop-list" 
sort -r -n +1 -2 "$OUTPUT_DIR/global.observations/WORD.T.lc1.sl0.mf2.count" | cut -f 1 | head -n 200 >"$OUTPUT_DIR/mt-200.stop-list" 
```

For this example we also create a small set of impostors in the same way as [presented in the GI method](user-guide-part1.html#preparing-the-impostors)):


```{bash gen.prep2}
source session-setup.sh
mkdir $OUTPUT_DIR/impostors
mkdir $OUTPUT_DIR/impostors/GI.1.impostors
n=$(find data/*/* -maxdepth 0 -type f | wc -l)
cd $OUTPUT_DIR/impostors/GI.1.impostors
find ../../../data/*/* -maxdepth 0 -type f | random-lines.pl 100 1 $n | while read f; do ln -s $f $(basename "$f").txt; done
```

A "resources" file is needed, it contains in particular the location of the stop words files and the impostors directory:

```{bash gen.prep3}
target="$OUTPUT_DIR/gen1/resources-options.conf"
echo "vocabResources=50:$OUTPUT_DIR/mt-50.stop-list;200:$OUTPUT_DIR/mt-200.stop-list" >"$target"
echo "useCountFiles=1" >>"$target"
echo "datasetResourcesPath=$OUTPUT_DIR/impostors/GI.1.impostors" >>"$target"
```
