---
title: "CLG authorship analytics - user guide (part 2)"
author: "Erwan Moreau"
date: "1/23/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Overview

This is the second part of the user guide for "CLG authorship analytics". The first part is [here](user-guide-part1.html).

Note: this guide is organized by order of complexity, from the most simple to the most complex kind of process. It is meant to help the reader understand what each of the different levels of the process actually does. Thus the simplest way to run the full process is described at the end of this document. 


## Rmd

This document was generated from an [R Markdown](https://rmarkdown.rstudio.com/) source file. The source file is provided in the repository, it can be used to reproduce these experiments. It can be executed through the RStudio interface ("knit" button) or as follows:

```
rmarkdown::render('user-guide-part2.Rmd')
```

- Naturally the document can also be read as a regular documentation.
- **Important:** this Rmd document includes [bash chunks](https://bookdown.org/yihui/rmarkdown/language-engines.html). In order for these to work, the environment must have been configured as explained in [Requirements](#requirements) below.

### Options

The `.Rmd` source document can be configured by modifying the following lines:

```{r myoptions}
packages.path <- 'packages'
Sys.setenv(CLG_AUTHORSHIP_PACKAGES_PATH = packages.path)
output.dir <- '/tmp/user-guide-part2.output'
Sys.setenv(OUTPUT_DIR = output.dir)
delete.previous.output <- TRUE
snippets.size <- 25
Sys.setenv(SNIPPETS_SIZE = snippets.size)
Sys.setenv(NCORES = 40)
Sys.setenv(TASKS_DIR= '/tmp/clg-authorship.tasks')
```

* Note: `ncores` should be set to represent the number of cores available on the machine.
* The role of `TASKS_DIR` is described below in the "Parallel/distributed processing" section.

In order to manually execute some of the commands below, it is recommended to assign the appropriate value to the above environment variables for the whole session, for example:

```{bash init0}
export CLG_AUTHORSHIP_PACKAGES_PATH=packages
export OUTPUT_DIR=/tmp/user-guide-part1.output
```

### Initialization

```{bash init1,echo=delete.previous.output,eval=delete.previous.output}
rm -rf "$OUTPUT_DIR" "$TASKS_DIR"
```

```{bash init2}
mkdir "$OUTPUT_DIR"
mkdir "$TASKS_DIR"
```

```{bash snippets}
source session-setup.sh
find data/gb/* data/ia/* -maxdepth 0 -type f | ./create-snippets-dataset.sh $SNIPPETS_SIZE $OUTPUT_DIR/data
```


# Full example

## Preparing the input data

The script `prepare-input-data.sh` can be used to create the working directory with all the required parts in a way which makes it usable directly by `train-top-level.sh`: 

* Copies the input data (to avoid modifying the input directory)
* Stores the multi-config files
* Calculates the stop words based on the training set documents (unless `-r` is supplied) and stores them where needed
** The "stop words limits" are obtained directly from the required obs types, assuming that the "stop word id" is a simple integer.
* Prepares all obs types for the input data, including POS if required
* The impostors data must be provided:
** either as a "prepared resource" with `-r`
** or as simple files in a directory with `-i`
*** in this case the impostors data is prepared, i.e. the required obs types are calculated
* In any case the impostors pre-similarity values are computed against every document in the input data
* The resources options file is generated

### Input data

* The input data dir must contain a file `truth.txt` containing all the training cases with their answer.

Below we build an example dataset (using both the training and test cases used previously):

```{bash full.prepa1}
mkdir "$OUTPUT_DIR"/input-data.full
cat data/user-guide.expl1/cases.txt data/user-guide.expl1/test-cases.txt | cut -f 1 | sed "s:data/gb:$OUTPUT_DIR/data:g" |  tr ' :' '\n\n'  | sort -u | while read f; do cp "$f" "$OUTPUT_DIR/input-data.full/"; done
cat data/user-guide.expl1/cases.txt data/user-guide.expl1/test-cases.txt | sed 's:data/[^/]*/::g' >"$OUTPUT_DIR/input-data.full/truth.txt"
```

### Impostors data

We build an impostors data:

```{bash full.prepa2}
source session-setup.sh
mkdir $OUTPUT_DIR/impostors.full
mkdir $OUTPUT_DIR/impostors.full/my.impostors
n=$(find $OUTPUT_DIR/data/* -maxdepth 0 -type f | wc -l)
find $OUTPUT_DIR/data/* -maxdepth 0 -type f | random-lines.pl 100 1 $n | while read f; do cp $f $OUTPUT_DIR/impostors.full/my.impostors/$(basename "$f").txt; done
```

- Note: the name `my.impostors` is the one used in the default GI config file `conf/GI.multi-conf.part`.

### Multi-conf files

For this example we create some simplified multi-conf files which are slightly more advanced than in the previous example:

- The three strategies are included.
- Observation types with stop word and POS tags are used.

**Important:** The "meta" config file must also be copied in the target directory under the name `meta-template.multi-conf`, as shown below.

```{bash multi2}
source session-setup.sh
mkdir "$OUTPUT_DIR/full"
generate-multi-conf.sh -c common.simplified2.multi-conf.part -g genetic.simplified.multi-conf.part 1 "$OUTPUT_DIR/full"
cat conf/meta-template.simplified.multi-conf > "$OUTPUT_DIR/full/meta-template.multi-conf"
```



### Running the preparation script


- Language: By default a file `contents.json` is expected in the input data dir (PAN input format), and the language is extracted from it. Option `-l` is used as an alternative to the json file.
- `-i` specifies where to find the impostors dataset(s)

```{bash prepare.input}
source session-setup.sh
prepare-input-data.sh -l english -i $OUTPUT_DIR/impostors.full/ "$OUTPUT_DIR/input-data.full" "$OUTPUT_DIR/full"
```


## Running the full training process with `train-top-level.sh`


Compared to `train-outerCV-1fold.sh`, The script `train-top-level.sh` can be used to run an additional level of cross-validation... although it does not actually perform a proper cross-validation:

- The training data is split randomly into two subsets  and B. `train-outerCV-1fold.sh` is called twice: 
     - With A as strategy training set and  as meta-training set + validation set, 
     - With B as strategy training set and A as meta-training set + validation set.
- After extracting/copying the best models (config and possibly Weka model) from both folds, the final bagging stage is performed on the whole training data. 
- The best models are sorted by their performance (depending on the selection method given as parameter, either on the validation set (meta-test set), full training set, or average of both) and written to a file named `best.results`, in which lines follow the format: `<Perf full training set> <Perf meta test set> <Average both>`.

This "top-level" stage can be useful to manually assess the effect of randomness on the results (to some extent). However the double genetic process is unlikely to return the exact same configurations in the two runs, so it is impossible in general to calculate the average performance of a particular configuration across the runs. 

The script receives only the previously initialized working directory, which contains the training data and multi-configuration files. 

```{bash full.run}
 source session-setup.sh
task-distrib-daemon.sh  -s 30s -p 1 -v -q 10 "$TASKS_DIR" $NCORES >"$OUTPUT_DIR/full.task-daemon.log" &
train-top-level.sh -r -P "$TASKS_DIR/mytasks" -o  '-c -s' "$OUTPUT_DIR/full"
```
