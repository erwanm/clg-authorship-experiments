---
title: "CLG authorship analytics - user guide (part 2)"
author: "Erwan Moreau"
date: "1/23/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Overview

This is the second part of the user guide for "CLG authorship analytics". The first part is [here](user-guide-part1.html).

Note: this guide is organized by order of complexity, from the most simple to the most complex kind of process. It is meant to help the reader understand what each of the different levels of the process actually does. Thus the simplest way to run the full process is described at the end of this document. 


## Rmd

This document was generated from an [R Markdown](https://rmarkdown.rstudio.com/) source file. The source file is provided in the repository, it can be used to reproduce these experiments. It can be executed through the RStudio interface ("knit" button) or as follows:

```
rmarkdown::render('user-guide-part2.Rmd')
```

- Naturally the document can also be read as a regular documentation.
- **Important:** this Rmd document includes [bash chunks](https://bookdown.org/yihui/rmarkdown/language-engines.html). In order for these to work, the environment must have been configured as explained in [Requirements](#requirements) below.

### Options

The `.Rmd` source document can be configured by modifying the following lines:

```{r myoptions}
packages.path <- 'packages'
Sys.setenv(CLG_AUTHORSHIP_PACKAGES_PATH = packages.path)
output.dir <- '/tmp/user-guide-part2.output'
Sys.setenv(OUTPUT_DIR = output.dir)
delete.previous.output <- TRUE
snippets.size <- 25
Sys.setenv(SNIPPETS_SIZE = snippets.size)
Sys.setenv(NCORES = 40)
Sys.setenv(TASKS_DIR= '/tmp/clg-authorship.tasks')
Sys.setenv(EXPE_WORK_DIR = 'experiments/1-doc-size')
```

* Note: `ncores` should be set to represent the number of cores available on the machine.
* The role of `TASKS_DIR` is described below in the "Parallel/distributed processing" section.

In order to manually execute some of the commands below, it is recommended to assign the appropriate value to the above environment variables for the whole session, for example:

```{bash init0}
export CLG_AUTHORSHIP_PACKAGES_PATH=packages
export OUTPUT_DIR=/tmp/user-guide-part1.output
```

### Initialization

```{bash init1,echo=delete.previous.output,eval=delete.previous.output}
rm -rf "$OUTPUT_DIR" "$TASKS_DIR"
```

```{bash init2}
mkdir "$OUTPUT_DIR"
mkdir "$TASKS_DIR"
```

```{bash snippets}
source session-setup.sh
find data/gb/* data/ia/* -maxdepth 0 -type f | ./create-snippets-dataset.sh $SNIPPETS_SIZE $OUTPUT_DIR/data
```


# Full example

This part shows the full process of training a meta-model with several strategies, albeit with simplified parameters. This experiment is self-contained, i.e. it doesn't rely on any part executed previously. See above for the detailed explanations.

## Preparing the input data

### Input data

We use the training set generated for the main experiments (see [Experiment 1: size of documents](expe1.html)).

```{bash full.prepa1}
mkdir "$OUTPUT_DIR"/input-data.full
cat "$EXPE_WORK_DIR/train.tsv" > "$OUTPUT_DIR/input-data.full/truth.txt"
cat "$EXPE_WORK_DIR/train.tsv"  | cut -f 1 | tr ' :' '\n\n'  | sort -u | while read f; do
      cp "$OUTPUT_DIR/data/$f" "$OUTPUT_DIR/input-data.full/"
done
```

## Impostors data

We build a small impostors dataset (same way as above):

```{bash full.prepa2}
source session-setup.sh
mkdir $OUTPUT_DIR/impostors.full
mkdir $OUTPUT_DIR/impostors.full/my.impostors
n=$(find $OUTPUT_DIR/data/* -maxdepth 0 -type f | wc -l)
find $OUTPUT_DIR/data/* -maxdepth 0 -type f | random-lines.pl 100 1 $n | while read f; do cp $f $OUTPUT_DIR/impostors.full/my.impostors/$(basename "$f").txt; done
```

- Note: the name `my.impostors` is the one used in the default GI config file `conf/GI.multi-conf.part`.

### Multi-conf files

For this example we create some simplified multi-conf files which are slightly more advanced than in the previous example:

- The three strategies are included.
- Observation types with stop word and POS tags are used.

**Important:** The "meta" config file must also be copied in the target directory under the name `meta-template.multi-conf`, as shown below.

```{bash full.multi2}
source session-setup.sh
mkdir "$OUTPUT_DIR/full"
generate-multi-conf.sh -c common.simplified2.multi-conf.part -g genetic.simplified.multi-conf.part 1 "$OUTPUT_DIR/full"
cat conf/meta-template.simplified.multi-conf > "$OUTPUT_DIR/full/meta-template.multi-conf"
```



### Running the preparation script


```{bash full.prepare.input}
source session-setup.sh
prepare-input-data.sh -l english -i $OUTPUT_DIR/impostors.full/ "$OUTPUT_DIR/input-data.full" "$OUTPUT_DIR/full"
```


## Running the full training process with `train-top-level.sh`


Compared to `train-outerCV-1fold.sh`, The script `train-top-level.sh` can be used to run an additional level of cross-validation... although it does not actually perform a proper cross-validation:

- The training data is split randomly into two subsets  and B. `train-outerCV-1fold.sh` is called twice: 
     - With A as strategy training set and  as meta-training set + validation set, 
     - With B as strategy training set and A as meta-training set + validation set.
- After extracting/copying the best models (config and possibly Weka model) from both folds, the final bagging stage is performed on the whole training data. 
- The best models are sorted by their performance (depending on the selection method given as parameter, either on the validation set (meta-test set), full training set, or average of both) and written to a file named `best.results`, in which lines follow the format: `<Perf full training set> <Perf meta test set> <Average both>`.

This "top-level" stage can be useful to manually assess the effect of randomness on the results (to some extent). However the double genetic process is unlikely to return the exact same configurations in the two runs, so it is impossible in general to calculate the average performance of a particular configuration across the runs. 

The script receives only the previously initialized working directory, which contains the training data and multi-configuration files. 

```{bash full.run}
source session-setup.sh
task-distrib-daemon.sh  -s 30s -p 1 -v -q 10 "$TASKS_DIR" $NCORES >"$OUTPUT_DIR/full.task-daemon.log" &
train-top-level.sh -r -P "$TASKS_DIR/mytasks" -o  '-c -s' "$OUTPUT_DIR/full"
```


## Extracting the best model 

At the end of the training process, the identifiers for the top N models/configurations (N=100 with the default parameters) found by the training process are written toa file `best.results` in the top output directory. The models are ordered by decreasing performance on the validation set (or meta test set; this depends on the parameters), thus showing the top model at the top.

* The three performance values available for every model in `best.results` are: 
    * performance on the validation set (meta test set)
    * performance on the full training set
    * average of the two above values
* Additional performance statistics are available for the top models in `bagging-meta-test-fold-all.stats`, which contains four values for every model: 
    * mean 
    * median 
    * standard deviation 
    * mean minus std. dev.


```{bash}
head "$OUTPUT_DIR/full/best.results"
```

The actual models corresponding to these identifiers are stored in the subdirectory `selected-meta-configs/`. The id contained in `best.results` actually corresponds to the full pathname prefix, so that:

* `<prefix>.conf` is the configuration file for the model
* `<prefix>.model>` is the directory containing the model.

If provided with such a patname prefix, the system can apply the corresponding model. Of course the path needs to exist and contain the model.


## Testing with fresh instances



### Preparing the test instances

When testing with fresh instances, the corresponding documents must have been "prepared" (their observation files computed, etc.) before applying the model per se. Of course this is not necessary if the documents in the test set have been prepared previously, together with the training documents (see above).

We use the test set generated for the main experiments (see [Experiment 1: size of documents](expe1.html)). Here we just copy the test files:

```{bash test.prepa1}
mkdir "$OUTPUT_DIR"/test-data.full
cat "$EXPE_WORK_DIR/test.tsv" > "$OUTPUT_DIR/test-data.full/truth.txt"
cat "$EXPE_WORK_DIR/test.tsv" | cut -f 1 | tr ' :' '\n\n'  | sort -u | while read f; do
      cp "$OUTPUT_DIR/data/$f" "$OUTPUT_DIR/test-data.full/input/"
done
```

Then we call the script `prepare-input-data.sh` on the directory containing the test documents:

```{bash test.prepare.input}
source session-setup.sh
prepare-input-data.sh -l english -r $OUTPUT_DIR/full/resources "$OUTPUT_DIR/test-data.full" "$OUTPUT_DIR/full.test"
```

- It's important to use the same impostors documents and list of stop words as for the training set. This is why the `-r` option allows to provide an exsting "resources" directory.
- Note: it's also possible to add the documents to the directory used for training.



### Applying the model to the test instances

Below the best model found by the training stage is extracted and its path prefix passed to the script `test-meta-model.sh`:

```{bash test.run}
source session-setup.sh
bestModel=$(head -n 1 "$OUTPUT_DIR/full/best.results" | cut -f 1)

task-distrib-daemon.sh  -s 30s -p 1 -v -q 10 "$TASKS_DIR" $NCORES >"$OUTPUT_DIR/test.task-daemon.log" &
train-top-level.sh -r -o  '-c -s' "$OUTPUT_DIR/full"

test-meta-model.sh -p  -P "$TASKS_DIR/mytasks" "$bestModel" "$EXPE_WORK_DIR/test.tsv" "$OUTPUT_DIR/full.test" "$OUTPUT_DIR/full.test.results" 
```

- The `-p` option requires computing performance. The script can also be used to only predict the answers for unlabelled cases.
